---
title: "CFA Analyses of the Science Interest Measure"
author: "Brandon Foster, Ph.D."
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output:
  html_document:
  html_notebook: null
---
# Introduction

The following report tests the fit of the hypothesized model for the Sciece Interest items in the Science Fairs Survey. The hypothesized model includes 3 dimensions, which are as follows:

- Desire to do science 

- Career interest in science

- Self-concept in science items

A total of 22 items were used for these analyses. The fit of the hypothesized model was examined using confirmatory factor analysis, whereby the factors are considered correlated with one another. Analyses will be run separately for the pre and post survey. Provided the models fit the data, the models will then be examined to see if the factor structure was invariant across time. All models were fit with the 'lavaan' package in R. 

## Import the data 

Let's bring the data into the system for analyses and load the packages necessary for analyses.  
```{r, import, message=FALSE, warning=FALSE}
# Load packaes ----
if (!require("pacman")) install.packages("pacman")
pacman::p_load(MplusAutomation, psych, tidyr, foreign, ggplot2, ggalt, ggthemes, 
    readr, dplyr, knitr, scales, pander, lavaan, kableExtra)

# Import the data ----
joined.dat <- readRDS(file = "../data/joined.dat")

# Load the custom functions ----
source("../functions/functions.r")

# Document options ----
panderOptions('digits', 2)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
```


# CFA of the science interest items (pre science fair exposure)

The code below sets up the relationships between indicators and constructs. 
```{r, cfa.preInt, message=FALSE, warning=FALSE}
# setup the model in lavaan 
m.pre.int <- ' 
desire  =~ s_preInt_Des_15 + s_preInt_Des_17 + s_preInt_Des_21 + s_preInt_Des_24 + s_preInt_Des_26 + s_preInt_Des_29 + s_preInt_Des_33

career  =~ s_preInt_Car_16 + s_preInt_Car_18 + s_preInt_Car_20 + s_preInt_Car_23 + s_preInt_Car_25 + s_preInt_Car_28 + s_preInt_Car_30 + s_preInt_Car_32 + s_preInt_Car_34 + s_preInt_Car_36

selfConcept  =~ s_preInt_Self_19 + s_preInt_Self_22 + s_preInt_Self_27 + s_preInt_Self_31 + s_preInt_Self_35
'
```

Next, the model is fit. In order to identify the models, the variances of latent variables set to 1. Missing data is handled with FIML. 
```{r, cfa.preInt.fit, message=FALSE, warning=FALSE}
# fit the model with: variances of latent variables set to 1, assuming correlated factors, and usinfg FIML to handle missing data
preInt.fit.1 <- cfa(m.pre.int, std.lv = TRUE, orthogonal = FALSE, data=joined.dat, missing='fiml')
```

Model fit statistics are provided to give a context for how the data fit the hypothesized model. In general, we like to see RMSEA < .05, and CFI & TLI > .90. The AIC and BIC are provided to compare this model to subsequent models, where the model containing the smallest AIC and BIC can be considered the better fitting of the two models. Results show that the model generally fits poorly. Specifically, the CFI and TLI values were below their minimally accepted criteria of .90, and the RMSEA value was above the criteria of .05. 

```{r, cfa.preInt.fit.stat, message=FALSE, warning=FALSE}
preInt.fit.1.measures <- fitMeasures(preInt.fit.1)
pander(tibble(
RMSEA = preInt.fit.1.measures[23],
Lower = preInt.fit.1.measures[24],
Upper = preInt.fit.1.measures[25],
CFI = preInt.fit.1.measures[9],
TLI = preInt.fit.1.measures[10],
AIC = preInt.fit.1.measures[19],
BIC = preInt.fit.1.measures[20]
))
```

Next, let's examine the factor loadings for the model. There is nothing in the magnitude of the factor loadings that are of concern. All the loadings are > .30, with most hovering around .60. 
```{r, cfa.preInt.fit.param, message=FALSE, warning=FALSE}
parameterEstimates(preInt.fit.1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, Beta=std.all) %>% 
  kable(digits = 3, format="html", caption="Factor Loadings") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Item residuals give a sense of how the hypothesized factors fit the correlation matrix between items. It is important to scrutinize items with residuals > 1.0. Results show that there was notable variance remaining in the indicators (i.e., > .10), after accounting for the factor, which is another indication that the model fit poorly. 
 
```{r, cfa.preInt.fit.res, message=FALSE, warning=FALSE}
parameterEstimates(preInt.fit.1, standardized=TRUE) %>% 
  filter(op == "~~") %>% 
  select('Indicator'=lhs, Indicator=rhs, 'Estiamte'=est) %>% 
  kable(digits = 3, format="html", caption="Factor Loadings") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Finally, modification indices are provided to give a sense of the what parameters in the model could be freed to improve model fit. Modification indicies showed that the mode could be improved by adding correlations between several items. Specifically, the largest potential modifications included: 

- residual correlations between s_preInt_Des_17	~~	s_preInt_Des_21
- residual correlations between s_preInt_Car_25	~~	s_preInt_Car_28
- residual correlations between s_preInt_Car_30	~~	s_preInt_Car_32
- cross-loading for self-concept on s_preInt_Car_34
- cross-loading for desire on s_preInt_Self_27

```{r, message=FALSE, warning=FALSE}
kable(modificationIndices(preInt.fit.1, sort.=TRUE, minimum.value=10), digits=3) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## Summmary of results

Overall, the model suggests that the hypothesized model does not adequately capture the relationships between the indicators. The postsence of such large residual correlations between items suggests the postsence of unspecified factors or content within items that overlaps with other dimensions specified in the model. 

# CFA of the science interest items (post science fair exposure)

Next, we will examine the CFA model for the items that were responded to during the post science fair exposure. 

The following sets up the relationships between indicators and constructs. 
```{r, cfa.postInt, message=FALSE, warning=FALSE}
m.post.int <- ' 
desire  =~ s_postInt_Des_15 + s_postInt_Des_17 + s_postInt_Des_21 + s_postInt_Des_24 + s_postInt_Des_26 + s_postInt_Des_29 + s_postInt_Des_33

career  =~ s_postInt_Car_16 + s_postInt_Car_18 + s_postInt_Car_20 + s_postInt_Car_23 + s_postInt_Car_25 + s_postInt_Car_28 + s_postInt_Car_30 + s_postInt_Car_32 + s_postInt_Car_34 + s_postInt_Car_36

selfConcept  =~ s_postInt_Self_19 + s_postInt_Self_22 + s_postInt_Self_27 + s_postInt_Self_31 + s_postInt_Self_35
'
```

Fit the model in lavaan with: variances of latent variables set to 1, assuming correlated factors, and using FIML to handle missing data. 
```{r, cfa.postInt.fit, message=FALSE, warning=FALSE}
# fit the model with: variances of latent variables set to 1, assuming correlated factors, and usinfg FIML to handle missing data
postInt.fit.1 <- cfa(m.post.int, std.lv = TRUE, orthogonal = FALSE, data=joined.dat, missing='fiml')
```

Model fit statistics showed that the model generally fits poorly. Specifically, the CFI and TLI values were below their minimally accepted criteria of .90, and the RMSEA value was > the criteria of .05. 

```{r, cfa.postInt.fit.stat, message=FALSE, warning=FALSE}
postInt.fit.1.measures <- fitMeasures(postInt.fit.1)
pander(tibble(
RMSEA = postInt.fit.1.measures[23],
Lower = postInt.fit.1.measures[24],
Upper = postInt.fit.1.measures[25],
CFI = postInt.fit.1.measures[9],
TLI = postInt.fit.1.measures[10],
AIC = postInt.fit.1.measures[19],
BIC = postInt.fit.1.measures[20]
))
```

Let's examine the factor loadings for the model. Again, eesults showed now discernible pattern in the magnitude of factor loadings in relation to the respective factors. 
```{r, cfa.postInt.fit.param, message=FALSE, warning=FALSE}
parameterEstimates(postInt.fit.1, standardized=TRUE) %>% 
  filter(op == "=~") %>% 
  select('Latent Factor'=lhs, Indicator=rhs, B=est, SE=se, Z=z, 'p-value'=pvalue, Beta=std.all) %>% 
  kable(digits = 3, format="html", caption="Factor Loadings") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


Item residuals showed that there was notable variance remaining in the indicators (i.e., > .10), after accounting for the factor, which is another indication that the model fit poorly. 
```{r, cfa.postInt.fit.res, message=FALSE, warning=FALSE}
parameterEstimates(postInt.fit.1, standardized=TRUE) %>% 
  filter(op == "~~") %>% 
  select('Indicator'=lhs, Indicator=rhs, 'Estiamte'=est) %>% 
  kable(digits = 3, format="html", caption="Factor Loadings") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Modification indicies showed that the mode could be improved by adding correlations between several items. Specifically, the largest potential modifications included: 

- adding a residual correlation between s_postInt_Car_16	and s_postInt_Car_20

- adding a residual correlation between s_postInt_Car_23	and	s_postInt_Car_32

- adding a residual correlation between s_postInt_Car_16	and s_postInt_Car_32

The modification indices also showed that several items cross-loaded onto other factors: 

- self-concept on s_postInt_Car_34

- desire on s_postInt_Des_33, s_postInt_Self_35, s_postInt_Car_32

- career on s_postInt_Des_15, s_postInt_Self_35

**Note.** These should only be added if they make sense substantively, and if you'd expect these correlations to exist in future data. 

```{r, message=FALSE, warning=FALSE}
kable(modificationIndices(postInt.fit.1, sort.=TRUE, minimum.value=10), digits=3) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


## Summmary of results

Overall, the model suggests that the hypothesized model does not adequately capture the relationships between the indicators. The postsence of such large residual correlations between items suggests the postsence of unspecified factors or content within items that overlaps with other dimensions specified in the model. Further, the overall model showed a degredation in fit when compared to the "pre" survey, which can be observerd through: worse model fit statistics, smaller factor loadings, and larger residual variances. This suggests that the factor structure is likely not invariant across time, though formal tests for this were not carried out.
