---
title: "Graded Response Models Science Interest Measure"
author: "Brandon Foster, Ph.D."
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output:
  html_document:
    simplex
  html_notebook: null
---

# Introduction

The following document explores the psychometric properties of the Science Interest Measure using a graded-response modeling (GSM). GSM is ideal for items with clear underlying response continuum (i.e., ordered categorical likert response). GMS models the probability of any given response category or higher (i.e., “cumulative logit model”), where the rating scale is split up into a series of binary categories (i.e., 0 vs. 1,2,3 | 0, 1 vs. 2,3, | 0, 1, 2 vs. 3 |, etc.). As a measurement model, transformed item responses are predicted using properties of persons (i.e., Theta) and properties of items (i.e., difficulty and discrimination). Two types of graded response models are considered in the analyses that follow. In the first the discrimination parameter is estimated, but considered fixed across items, which is analogous to a 1-PL pseudo 1-PL model. As a contrast, the Andrich rating scale model fixes the discrimination at 1 across all items. In the second set of GSM, the discrimination parameter is allowed to vary across items. 

# Data Screeining 

First, let's bring in the data and munge it appropriately. 
```{r, import, message=FALSE, warning=FALSE, tidy=TRUE}
# Set working directory ----
#setwd("/Users/bfoster/Desktop/2017-edc/science-fairs-analyses")
# Load packaes ----
if (!require("pacman")) install.packages("pacman")
pacman::p_load(psych, ggplot2, ggalt, ggthemes, 
    readr, dplyr, knitr, scales, pander, kableExtra, stringr, scales,
    mirt, ltm, tidyverse, formattable, gridExtra)

# Import the data ----
joined.dat <- readRDS(file = "../data/joined.dat")

# Munge data ----
items <- joined.dat %>%
  dplyr::select(StudentID, s_preInt_Des_15, s_preInt_Des_17, s_preInt_Des_21, s_preInt_Des_24,
                s_preInt_Des_26, s_preInt_Des_29, s_preInt_Des_33, s_preInt_Car_16,
                s_preInt_Car_18, s_preInt_Car_20, s_preInt_Car_23, s_preInt_Car_25,
                s_preInt_Car_28, s_preInt_Car_30, s_preInt_Car_32, 
                s_preInt_Car_34, s_preInt_Car_36, s_preInt_Self_19, s_preInt_Self_22, 
                s_preInt_Self_27, s_preInt_Self_31 ,s_preInt_Self_35) %>%
  rename_(.dots=setNames(names(.), gsub("s_preInt_", "", names(.))))

# munge the variable names to remove pre_ and post_ prefixes
items.stringr.prune <- items %>% 
    mutate(
      Des_15 = Des_15 - 1, 
      Des_17 = Des_17 - 1,
      Des_21 = Des_21 - 1,
      Des_24 = Des_24 - 1, 
      Des_26 = Des_26 - 1,
      Des_29 = Des_29 - 1,
      Des_33 = Des_33 - 1,
      Car_16 = Car_16 - 1,
      Car_18 = Car_18 - 1,
      Car_20 = Car_20 - 1,
      Car_23 = Car_23 - 1,
      Car_25 = Car_25 - 1,
      Car_28 = Car_28 - 1,
      Car_30 = Car_30 - 1,
      Car_32 = Car_32 - 1, 
      Car_34 = Car_34 - 1,
      Car_36 = Car_36 - 1,
      Self_19 = Self_19 - 1,
      Self_22 = Self_22 - 1, 
      Self_27 = Self_27 - 1,
      Self_31 = Self_31 - 1,
      Self_35 = Self_35 - 1)

# create dataframe for item reference
item.ref <- tibble(
  Item = colnames(items.stringr.prune)[-1],
  Number = 1:22)
```

# Item Descriptives

1. Syntax below creates the item statistics using the `ltm` packages, and conducts all necessary munging for printing tables and plots. 

```{r, descriptives.pre, message=FALSE, warning=FALSE, tidy=TRUE}
# easy item descriptive statistics from the 'ltm' package
pre.items.descriptives <- descript(items[-1], chi.squared = TRUE, 
                                   B = 1000)
# extract the proportions in each categoty
pre.per <- do.call(rbind, lapply((pre.items.descriptives[2]), data.frame, 
                                 stringsAsFactors=FALSE)) %>%
  mutate(item = colnames(items.stringr.prune)[-1]) %>%
         rename(Cat1 = X1, Cat2 = X2, Cat3 = X3, Cat4 = X4, Cat5 = X5) %>%
  dplyr::select(item, Cat1, Cat2, Cat3, Cat4, Cat5)

# convert to long for plotting 
pre.per.long <- gather(pre.per, cat, value, -item) %>%
  arrange(item)
```

2. Let's look at the percent of missing responses for each item. A color bar has been added to the values in the table to compare the relative proportion missing per each item. 
```{r, descriptives.1, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
# extract the proportions in each categoty
do.call(rbind, lapply((pre.items.descriptives[7]), data.frame, 
                                 stringsAsFactors=FALSE)) %>%
  rownames_to_column("Statistic") %>%
  filter(Statistic=="missin.(%)") %>%
  gather(item, value, -Statistic) %>% 
  dplyr::select(item, value) %>%
  rename(Percent = value, Item = item) %>% 
  mutate("Percent Missing" = color_bar("lightgreen")(Percent)) %>%
  dplyr::select(Item, "Percent Missing") %>%
  kable(digits = 2, format="html", caption="Category Utilization for Pre-Administration 
        Period", escape = F) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```

3. Let's look at the table of the proportions of rating scale category utilization to see if anything looks aberrant. The table shows that for most items the utilization of the lower categories is low. This might pose problems downstream when the category probability curves are examined. 

```{r, descriptives.2, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
# print the table
pre.per %>%
  rename(Item = item) %>%
  kable(digits = 3, format="html", caption="Category Utilization for Pre-Administration 
        Period") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

4. A visualization is provided for another perspective to examine category utilization. 
```{r, descriptives.3, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
# plot the proportions
p_pl1_prop <- ggplot() + geom_bar(aes(y = value, x = item, fill = cat), 
                                  data = pre.per.long, stat="identity") +
  ggtitle("Proportion of Category Utilization") + 
  scale_fill_ptol() + theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
p_pl1_prop
```

5. A histogram of the total score. You see a spike with people near the complete total of 110, as well as a long left tail. 
```{r, descriptives.4, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
total <- rowSums(items[-1])
histogram(~total, breaks=50)
```

6. Examine the CTT reliability statistics (i.e., alpha and alpha-if-removed for each item). 
```{r, descriptives.5, message=FALSE, warning=FALSE, tidy=TRUE}
# extract the proportions in each categoty
item.names.alpa <- c("Total Alpha", colnames(items)[-1])
do.call(rbind, lapply((pre.items.descriptives[9]), data.frame, 
                                 stringsAsFactors=FALSE)) %>%
  mutate(Item = item.names.alpa) %>%
  dplyr::select(Item, value) %>%
  kable(digits = 2, format="html", caption="Alpha for Pre-Administration 
        Period", escape = F) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 
```

# Fit the GSM models

1. The syntax below fits both the 1-PLish and 2-PL GSM model. The syntax block concludes by comparing the fit of one model versus the other in order to decide which model to utilize moving forward. Standard errors are calculated based on the sandwich covariance estimate, which was chosen to adjust for nesting in the data. Results for the comparison between models, using the `anova()` and the `M2()` functions, showed that the 2-PL was a better fitting model to the item responses. As such, this model was utilized in followup analyses. 

```{r, mirt.1, message=FALSE, warning=FALSE, tidy=TRUE}
# define the number of cores for quicker processing 
mirtCluster(5)  

# MIRT syntax for 1-PLish model with fixed discrimination 
mod.syntax_1pl_fixed <- '
THETA=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22
CONSTRAIN = (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,a1)
COV = THETA*THETA
'

mirt_1pl_fixed  <- mirt.model(mod.syntax_1pl_fixed) # make the MIRT model part 2

# run the mirt 1-PLish model
model_1pl_fixed <- mirt(items[-1], mirt_1pl_fixed, itemnames = 
                        c(colnames(items[-1])), 
                        itemtype = 'graded', 
                        technical = list(removeEmptyRows=TRUE, parallel=TRUE), 
                        empiricalhist = TRUE,
                        SE = TRUE, SE.type = 'sandwich')

# MIRT syntax for 2-PL model with fixed discrimination 
mod.syntax_2pl <- '
THETA=1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22
COV = THETA*THETA
'

mirt_2pl <- mirt.model(mod.syntax_2pl) # make the MIRT model part 2

# run the mirt 2-PL GSM model
model_2pl <- mirt(items[-1], 1, itemnames = c(colnames(items[-1])), 
                  itemtype = 'graded', 
                  technical = list(removeEmptyRows=TRUE, parallel=TRUE), 
                  empiricalhist = TRUE, 
                  SE = TRUE, SE.type = 'sandwich')

# run the mirt 2-PL GRSM model
model_2pl_grsm <- mirt(items[-1], 1, itemnames = c(colnames(items[-1])), 
                  itemtype = 'grsmIRT', 
                  technical = list(removeEmptyRows=TRUE, parallel=TRUE), 
                  empiricalhist = TRUE, 
                  SE = TRUE, SE.type = 'sandwich')

# run the mirt 2-PL GPCM model
model_2pl_gpcm <- mirt(items[-1], 1, itemnames = c(colnames(items[-1])), 
                  itemtype = 'gpcmIRT', 
                  technical = list(removeEmptyRows=TRUE, parallel=TRUE), 
                  empiricalhist = TRUE, 
                  SE = TRUE, SE.type = 'sandwich')

# test the fit of 1 model vs. the other w/ BIC Table
tibble(
  model_2pl_gpcm = model_2pl_gpcm@Fit$BIC,
  model_2pl_grsm = model_2pl_grsm@Fit$BIC,
  model_2pl_graded = model_2pl@Fit$BIC,
  model_1pl_fixed = model_1pl_fixed@Fit$BIC)%>%
  gather(key, BIC)%>%
  arrange(BIC)
# summary(model_2pl)
# coef(model_2pl)
# wald(model_2pl)
# plot(model_2pl)
# fitted(model_2pl)
# residuals(model_2pl)
# expected.item(model_2pl)
```

2. Inspect the best model using coef(), plotting functions and goodness-of-fit functions (see the according slide and cheat sheet). The model fit statistics show that Car_36 and Car_25 showed the highest discrimination. A number of self-concept in science items had a low discrimination, notably Self_31, Self_22, Self_19 and Self_35. The difficulty estimates for the categories for these items indicate that they are relatively easy for most of the sample. 

The category probability curves were examined for the items with the highest discriminations (i.e., Car_36, Car_32, Car_25, Des_33, Car_18, Car_30). Most items yielded unique peaks for each category, with the exception of Car_30, where Category 4 was buried between 3 and 5. The unique range of the construct captured by the curves for the middle categories of the rating scale were only moderately pronounced. Across all items, the middle categories covered a focal range of the latent construct from approximately theta = - 2.0 to .5, indicating that these categories were capable of discriminating between students with lower levels of the construct. The plot for the category item difficulties also showed that these items all had the same approximate difficulties for the categories. 

The trace plots for the rest of the items were also examined, and a general deficiency was observed for categories 2 and 4. There either needs to be fewer categories in the rating scale or the categories need to be better defined in the item prompts.  The expected item scoring curves were also examined. Notable was that the discrimination for all of the self items was quite poor. This might suggest that these items should be utilized as their own measure, which would make sense given the content of the items, but would differ from the results that emerged in the CFA analyses. The information provided by these items was also quite poor. In addition, the information provided by Des_17, Des_21, Des_24, and Des_26 was also poor.  

```{r, mirt.2, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
as.data.frame(coef(model_2pl, IRTparms = T, simplify = TRUE)) %>%
  rename(discrimination = items.a1,
         difficulty_cat1_2345 = items.d1,
         difficulty_cat12_345 = items.d2,
         difficulty_cat123_45 = items.d3,
         difficulty_cat1234_5 = items.d4) %>%
  mutate(Item = colnames(items)[-1]) %>%
  dplyr::select(Item, discrimination, difficulty_cat1_2345, difficulty_cat12_345, 
         difficulty_cat123_45, difficulty_cat1234_5) %>%
  arrange(-discrimination) %>%
  kable(digits = 2, format="html", caption="Item IRT Parameters", escape = F) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 


# Plot the category probability curves for highest discriminations
p.36 <- itemplot(model_2pl, "Car_36", type = 'infotrace')
p.36
p.32 <- itemplot(model_2pl, "Car_32", type = 'infotrace')
p.32
p.25 <- itemplot(model_2pl, "Car_25", type = 'infotrace')
p.25
p.33 <- itemplot(model_2pl, "Des_33", type = 'infotrace')
p.33
p.18 <- itemplot(model_2pl, "Car_18", type = 'infotrace')
p.18
p.30 <- itemplot(model_2pl, "Car_30", type = 'infotrace')
p.30

# coef(model_1pl_fixed, simplify = TRUE) 
irtParms_model_2pl <- coef(model_2pl, IRTpars = TRUE, simplify = TRUE) 
irtParms_model_2pl <- as.data.frame(irtParms_model_2pl$items)
irtParms_model_2pl <- cbind(irtParms_model_2pl, items = colnames(items[-1])) 

# convert wide to long
irtParms_model_2pl_long <- irtParms_model_2pl %>%
  gather(param, value, -items) %>%
  slice(23:110)

# plot the difficulties
ggplot(irtParms_model_2pl_long, aes(x = items, y = value, color = param, group = param)) + 
  geom_point()  + 
  theme_minimal() + scale_color_calc() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Category difficulties", 
       subtitle = "1-PL Graded Response Model with fixed discriminiation",
       x = "Items",
       y = "Category Difficulty",
       color = "Categories")

# plot trace lines for all items
plot(model_2pl, type = 'trace')

# plot the item score for all items
plot(model_2pl, type = "itemscore", facet_items = TRUE) 

# plot the information curves for each item
plot(model_2pl, type = "infotrace", facet_items = T) 
```  

3. How well do these models fit the data, and do the items appear to behave well given the selected itemtypes? The `M2()` function is used to assess global model fit, while the `itemfit()` in a piece-wise assessment of fit to discover where the model is misfitting. The `itemfit()` reports (Kang and Chen's (2007) Generalized S-X^2 Item-Fit Index)[http://files.eric.ed.gov/fulltext/ED510479.pdf]. In addition, the `p.adjust()` function with argument `method="fdr"` to correct for multiple testing of the itemfit() function.   

Criteria for evaluating overall model fit:

- RMSEA2 (i.e., bivariate RMSEA), where a value <= a threshold of .05/k-1 (i.e., 0.092) indicates good fit. 

- SRMR <= .05

- Non-significant M2 statistic

Results for the global model fit showed that the model fit could be improved. The M2 value was significant. The SRMR was also above was also > .05. However, the RMSEA2 value was < 0.092. The piece-wise examination of the fit of the items using the S-X^2 statistic showed that Car_28 potentially misfit. In examining the probability of category utilization table for the item, it appears like most people are utilizing the first and second category. The observed vs. expected values were also examined at several points along the continuum of the estimated thetas. Some misfit was observed (i.e., zstd values 1.96) for those with low levels of the latent trait, as well as those with slightly above average levels of the trait. 

```{r, mirt.3, message=FALSE, warning=FALSE, tidy=TRUE}
# Global fit ---

# M2
M2_2PL <- M2(model_2pl, impute = 100)
M2_2PL

# compute the sample bivariate RMSEA
sqrt((M2_2PL$M2[1]-M2_2PL$df[1])/(477*M2_2PL$df[1]))

# Observed vs. expected for misfitting items 
items.na <- na.omit(items) # df w/out missing

# refit feeding the parameter estiamtes from original model 
model_2pl.na <- mirt(items.na[-1], mirt_2pl, itemnames = c(colnames(items.na[-1])),
                  itemtype = 'graded', 
                  pars=mod2values(model_2pl), TOL=NaN)

# Piecewise misfit ---

# item fit statistics 
itemfit_2pl <- itemfit(model_2pl, impute = 100) 

# apply a false discovery rate to the p-value 
# p.fdr <- p.adjust(itemfit_2pl$p.S_X2,"BH")
# itemfit_2pl <- cbind(itemfit_2pl, p.fdr) # bind to previous work

# sort the item fit statistics by p-value
itemfit_2pl %>%
  slice(24:45) %>% 
  arrange(p.S_X2) %>%
  kable(digits = 2, format="html", caption="Item Fit Statistics", escape = F) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed")) 

# examining Car_28
p.28 <- itemplot(model_2pl, "Car_28", type = 'trace')
p.28

# exmaine the category utilization for Car_28
pre.per %>%
  filter(item=="Car_28")

# Observed vs. expected plots for category utilization
items.na <- na.omit(items) # df w/out missing

# refit feeding the parameter estiamtes from original model 
model_2pl.na <- mirt(items.na[-1], mirt_2pl, itemnames = c(colnames(items.na[-1])),
                  itemtype = 'graded', 
                  pars=mod2values(model_2pl), TOL=NaN)
itemfit_Car_32 <- itemfit(model_2pl.na, empirical.plot=13) # Car_28 (misfit Cat2 and Cat3)
itemfit_Car_32

# examine the observed vs. expected values looking for ZSTD with absolute value 1.96. 
itemfit(model_2pl, empirical.table = 13)
```

4. Examine information, SEM, and reliability for the whole measure. The plots show that the measure captures the most information from theta -2 to +2. No surprise that SEM was lowest in this range. Reliability was highest between -4 to 2.25. 
```{r, mirt.4, message=FALSE, warning=FALSE, tidy=TRUE, results='asis'}
# examine test information
info_model_2pl <- tibble(
  theta = seq(-6,6,.01),
  information = testinfo(model_2pl, theta),
  error = 1/sqrt(information),
  reliability = information/(information+1))

# plot test information
plot(model_2pl, type='info', MI=100)

# plot SEM
plot(model_2pl, type='SE', MI=100)

# plot alpha at theta levels
plot(model_2pl, type='rxx', MI=100)
```

# Factor scores vs Standardized total scores 
```{r}
# Factor scores vs Standardized total scores 
fs <- as.vector(fscores(model_2pl.na, method = "WLE")) 
sts <- as.vector(scale(apply(na.omit(items)[-1], 1, sum))) 
plot(fs ~ sts) 

```

